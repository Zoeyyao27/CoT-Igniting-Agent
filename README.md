# Sparks of Language Intelligence: From Chain-of-Thought Reasoning to Language Agent Automation

![CoT_overview](\fig\CoT_overview.png)

üëâüèªThis repository contains the paper list for the paper: Sparks of Language Intelligence: From Chain-of-Thought Reasoning to Language Agent Automation. 

üëÄPlease check out our paper for more informationÔºÅü´°

## Contents

[TOC]

## Paradigm Shifts of CoT

### 1. Prompting Paradigm

#### 1.1 Instruction Generation

- **Zero-Shot-CoT**

  [2022.05]  Large language models are zero-shot reasoners [[paper]](https://arxiv.org/abs/2205.11916)

  Kojima T, Gu S S, Reid M, et al. NIPS 2022.

- **Plan-and-solve prompting**

  [2023.05] Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models [[paper]](https://arxiv.org/abs/2305.04091)

  Wang L, Xu W, Lan Y, et al. arXiv. 

- **Automatic Prompt Engineer**

  [2022.11] Large language models are human-level prompt engineers [[paper]](https://arxiv.org/abs/2211.01910)
  
  Zhou Y, Muresanu A I, Han Z, et al.  arXiv. 

- **OPRO**

  [2023.09] Large language models as optimizers [[paper]](https://arxiv.org/abs/2309.03409)

  Yang C, Wang X, Lu Y, et al. arXiv. 

#### 1.2 Exemplar Generation

- **Manual-CoT**

  [2022.01] Chain-of-thought prompting elicits reasoning in large language models [[paper]](https://arxiv.org/abs/2201.11903)

  Wei J, Wang X, Schuurmans D, et al.  NIPS 2022.

- **Active-Prompt**

  [2023.02] Active prompting with chain-of-thought for large language models [[paper]](https://arxiv.org/abs/2302.12246)

  Diao S, Wang P, Lin Y, et al.  arXiv.

- **Auto-CoT**

  [2022.10] Automatic chain of thought prompting in large language models [[paper]](https://arxiv.org/abs/2210.03493)

  Zhang Z, Zhang A, Li M, et al. arXiv.

- **Automate-CoT**

  [2023.02] Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data [[paper]](https://arxiv.org/abs/2302.12822)
  
  Shum K S, Diao S, Zhang T. arXiv.

### 2. CoT Reasoning

#### 2.1 CoT Formulation

![image-20231012211541542](\fig\CoT_formulation.png)

- **Program-of-thoughts**

  [2022.11] Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks [[paper]](https://arxiv.org/abs/2211.12588)

  Chen W, Ma X, Wang X, et al. arXiv.

- **Tab-CoT**

  [2023.05] Tab-CoT: Zero-shot Tabular Chain of Thought [[paper]](https://aclanthology.org/2023.findings-acl.651/)

  Jin Z, Lu W.  ACL 2023 findings

- **Tree-of-Thoughts**

  [2023.05]  Tree of thoughts: Deliberate problem solving with large language models [[paper]](https://arxiv.org/abs/2305.10601)

  Yao S, Yu D, Zhao J, et al. arXiv.

- **Graph-of-Thought (Rationale)**

  [2023.08] Graph of thoughts: Solving elaborate problems with large language models [[paper]](https://arxiv.org/abs/2308.09687)

  Besta M, Blach N, Kubicek A, et al. arXiv.

- **Skeleton-of-thought**

  [2023.07] Skeleton-of-thought: Large language models can do parallel decoding [[paper]](https://arxiv.org/abs/2307.15337)

  Ning X, Lin Z, Zhou Z, et al. arXiv.

- **Recursion of Thought**

  [2023.06] Recursion of Thought: A Divide-and-Conquer Approach to Multi-Context Reasoning with Language Models [[paper]](https://aclanthology.org/2023.findings-acl.40.pdf)

  Lee S, Kim G. ACL 2023 findings.

#### 2.2 Reasoning Aggregation

- **Rationale-Augmented Ensembles**

  [2022.07] Rationale-augmented ensembles in language models [[paper]](https://arxiv.org/abs/2207.00747)

  Wang X, Wei J, Schuurmans D, et al. arXiv.

- **Self-consistency** **CoT**

  [2022.03] Self-consistency improves chain of thought reasoning in language models [[paper]](https://arxiv.org/abs/2203.11171)

  Wang X, Wei J, Schuurmans D, et al.  ICLR 2023.

#### 2.3 CoT Verification

- **Natural Program**

  [2023.06] Deductive Verification of Chain-of-Thought Reasoning [[paper]]()

  Ling Z, Fang Y, Li X, et al. arXiv.

-  **PRM**

  [2023.05] Let's Verify Step by Step [[paper]](https://arxiv.org/abs/2305.20050)

  Lightman H, Kosaraju V, Burda Y, et al. arXiv.

- **Self-Verification**

  [2022.12] Large language models are better reasoners with self-verification [[paper]](https://arxiv.org/abs/2212.09561)

  Weng Y, Zhu M, Xia F, et al. arXiv.

- **CRITIC**

  [2022.12] Critic: Large language models can self-correct with tool-interactive critiquing [[paper]](https://arxiv.org/abs/2305.11738)

  Gou Z, Shao Z, Gong Y, et al. arXiv.

- **Verify-and-Edit** üõ†Ô∏è

  [2023.05] Verify-and-edit: A knowledge-enhanced chain-of-thought framework [[paper]](https://aclanthology.org/2023.acl-long.320/)

  Zhao R, Li X, Joty S, et al. ACL 2023.

- **AuRoRA**

  [2023.08] AuRoRA: Augmented Reasoning and Refining with Task-Adaptive Chain-of-Thought Prompting [[website]]({https://anni-zou.github.io/aurora-en.github.io/)

  Zou A, Zhang Z, Zhao H

### 3. CoT Application

#### 3.1 CoT Extension üñºÔ∏è

![image-20231012211643802](\fig\CoT_extension.png)

- **Multilingual-CoT**

  [2022.10] Language models are multilingual chain-of-thought reasoners [[paper]](https://arxiv.org/abs/2210.03057)

  Shi F, Suzgun M, Freitag M, et al. ICLR 2023.

- **Multimodal-CoT**

  [2023.02] Multimodal chain-of-thought reasoning in language models [[paper]](https://arxiv.org/abs/2302.00923)
  
  Zhang Z, Zhang A, Li M, et al. arXiv.
  
- **Graph-of-Thought (Input)**

  [2023.05] Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Large Language Models [[paper]](https://arxiv.org/abs/2305.16582)
  
  Yao Y, Li Z, Zhao H. arXiv.

#### 3.2 CoT for Classic NLP Task üìñ

- **SumCoT**

  [2023.05] Element-aware Summarization with Large Language Models: Expert-aligned Evaluation and Chain-of-Thought Method [[paper]](https://aclanthology.org/2023.acl-long.482/)

  Wang Y, Zhang Z, Wang R.  ACL 2023.

- **Self-Prompting**

  [2022.12] Self-prompting large language models for open-domain qa [[paper]](https://arxiv.org/abs/2212.08635)
  
  Li J, Zhang Z, Zhao H. arXiv.

#### 3.3 CoT for Agentü§ñ

![image-20231012215933317](\fig\CoT_agent.png)

- **ReAcT**

  [2022.10] React: Synergizing reasoning and acting in language models [[paper]]()

  Yao S, Zhao J, Yu D, et al. ICLR 2023„ÄÇ

- **Android in the Wild**

  [2023.07] Android in the Wild: A Large-Scale Dataset for Android Device Control [[paper]](https://arxiv.org/abs/2307.10088)

  Rawles C, Li A, Rodriguez D, et al. arXiv.

- **ToolLLM**

  [2023.07] Toolllm: Facilitating large language models to master 16000+ real-world apis [[paper]](https://arxiv.org/abs/2307.16789)

  Qin Y, Liang S, Ye Y, et al. arXiv.

- **MM-ReAcT**

  [2023.03] Mm-react: Prompting chatgpt for multimodal reasoning and action [[paper]](https://arxiv.org/abs/2303.11381)
  
  Yang Z, Li L, Wang J, et al. arXiv.

#### 3.4 CoT for Science üß™

- **ChemCrow**

  [2023.04] ChemCrow: Augmenting large-language models with chemistry tools[[paper]](https://arxiv.org/abs/2304.05376)

  Bran A M, Cox S, White A D, et al. arXiv.

- **Med-PaLM**

  [2022.12] Large language models encode clinical knowledge [[paper]](https://www.nature.com/articles/s41586-023-06291-2)
  
  Singhal K, Azizi S, Tu T, et al.  Nature, 2023.

### 4. CoT Explanation

- **Implicit Bayesian Inference**

  [2021.11] An explanation of in-context learning as implicit bayesian inference [[paper]](https://arxiv.org/abs/2111.02080)

  Xie S M, Raghunathan A, Liang P, et al. arXiv.

- **Locality of Experience**

  [2023.04] Why think step-by-step? Reasoning emerges from the locality of experience [[paper]](https://arxiv.org/abs/2304.03843)
  
  Prystawski B, Goodman N D. arXiv.

### 5. CoT Safety

- **Faithful** **CoT**

  [2023.01] Faithful chain-of-thought reasoning [[paper]](https://arxiv.org/abs/2301.13379)

  Lyu Q, Havaldar S, Stein A, et al. IJCNLP-AACL 2023

- **Bias** **and Toxicity**

  [2023.01] On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning [[paper]](https://aclanthology.org/2023.acl-long.244/)
  
  Shaikh O, Zhang H, Held W, et al. ACL 2023



## Citation

