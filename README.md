# Sparks of Language Intelligence: From Chain-of-Thought Reasoning to Language Agent Automation

![CoT_overview](fig\CoT_overview.png)

üëâüèªThis repository contains the paper list for the paper: Sparks of Language Intelligence: From Chain-of-Thought Reasoning to Language Agent Automation. 

üëÄPlease check out our paper for more informationÔºÅü´°

## Contents

[TOC]



## Paradigm Shifts of CoT

### 1. Prompting Paradigm

#### 1.1 Instruction Generation

- **Zero-Shot-CoT**

  [2022.05]  Large language models are zero-shot reasoners [[paper]](https://arxiv.org/abs/2205.11916)

  Kojima T, Gu S S, Reid M, et al. NIPS 2022.

- [2023.05] Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models [[paper]](https://arxiv.org/abs/2305.04091)

  Wang L, Xu W, Lan Y, et al. arXiv. 

- [2022.11] Large language models are human-level prompt engineers [[paper]](https://arxiv.org/abs/2211.01910)

  Zhou Y, Muresanu A I, Han Z, et al.  arXiv. 

#### 1.2 Exemplar Generation

- [2022.01] Chain-of-thought prompting elicits reasoning in large language models [[paper]](https://arxiv.org/abs/2201.11903)

  Wei J, Wang X, Schuurmans D, et al.  NIPS 2022.

- [2023.02] Active prompting with chain-of-thought for large language models [[paper]](https://arxiv.org/abs/2302.12246)

  Diao S, Wang P, Lin Y, et al.  arXiv.

- [2022.10] Automatic chain of thought prompting in large language models [[paper]](https://arxiv.org/abs/2210.03493)

  Zhang Z, Zhang A, Li M, et al. arXiv.

- [2023.02] Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data [[paper]](https://arxiv.org/abs/2302.12822)

  Shum K S, Diao S, Zhang T. arXiv.

### 2. CoT Reasoning

#### 2.1 CoT Verification

- [2023.06] Deductive Verification of Chain-of-Thought Reasoning [[paper]]()

  Ling Z, Fang Y, Li X, et al. arXiv.

- [2023.05] Let's Verify Step by Step [[paper]](https://arxiv.org/abs/2305.20050)

  Lightman H, Kosaraju V, Burda Y, et al. arXiv.

- [2022.12] Large language models are better reasoners with self-verification [[paper]](https://arxiv.org/abs/2212.09561)

  Weng Y, Zhu M, Xia F, et al. arXiv.

- [2022.12] Critic: Large language models can self-correct with tool-interactive critiquing [[paper]](https://arxiv.org/abs/2305.11738)

  Gou Z, Shao Z, Gong Y, et al. arXiv.

- [2023.05] Verify-and-edit: A knowledge-enhanced chain-of-thought framework [[paper]](https://aclanthology.org/2023.acl-long.320/)

  Zhao R, Li X, Joty S, et al. ACL 2023.

#### 2.2 CoT Formulation

- [2022.11] Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks [[paper]](https://arxiv.org/abs/2211.12588)

  Chen W, Ma X, Wang X, et al. arXiv.

- [2023.05] Tab-CoT: Zero-shot Tabular Chain of Thought [[paper]](https://aclanthology.org/2023.findings-acl.651/)

  Jin Z, Lu W.  ACL 2023 findings

- [2023.05]  Tree of thoughts: Deliberate problem solving with large language models [[paper]](https://arxiv.org/abs/2305.10601)

  Yao S, Yu D, Zhao J, et al. arXiv.

- [2023.05] Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Large Language Models [[paper]](https://arxiv.org/abs/2305.16582)

  Yao Y, Li Z, Zhao H. arXiv.

- [2023.07] Skeleton-of-thought: Large language models can do parallel decoding[[paper]](https://arxiv.org/abs/2307.15337)

  Ning X, Lin Z, Zhou Z, et al. arXiv.

- [2023.06] Recursion of Thought: A Divide-and-Conquer Approach to Multi-Context Reasoning with Language Models[[paper]](https://aclanthology.org/2023.findings-acl.40.pdf)

  Lee S, Kim G. ACL 2023 findings.

- [2022.07] Rationale-augmented ensembles in language models [[paper]](https://arxiv.org/abs/2207.00747)

  Wang X, Wei J, Schuurmans D, et al. arXiv.

- [2022.03] Self-consistency improves chain of thought reasoning in language models [[paper]](https://arxiv.org/abs/2203.11171)

  Wang X, Wei J, Schuurmans D, et al.  ICLR 2023.

### 3. CoT Application

#### 3.1 CoT Extension üñºÔ∏è

- [2022.10] Language models are multilingual chain-of-thought reasoners [[paper]](https://arxiv.org/abs/2210.03057)

  Shi F, Suzgun M, Freitag M, et al. ICLR 2023.

- [2023.02] Multimodal chain-of-thought reasoning in language models [[paper]](https://arxiv.org/abs/2302.00923)

  Zhang Z, Zhang A, Li M, et al. arXiv.

#### 3.2 CoT for Classic NLP Task üìñ

- [2023.05] Element-aware Summarization with Large Language Models: Expert-aligned Evaluation and Chain-of-Thought Method [[paper]](https://aclanthology.org/2023.acl-long.482/)

  Wang Y, Zhang Z, Wang R.  ACL 2023.

- [2022.12] Self-prompting large language models for open-domain qa [[paper]](https://arxiv.org/abs/2212.08635)

  Li J, Zhang Z, Zhao H. arXiv.

#### 3.3 CoT for Agentü§ñ

- [2022.10] React: Synergizing reasoning and acting in language models [[paper]]()

  Yao S, Zhao J, Yu D, et al. ICLR 2023„ÄÇ

- [2023.07] Android in the Wild: A Large-Scale Dataset for Android Device Control [[paper]](https://arxiv.org/abs/2307.10088)

  Rawles C, Li A, Rodriguez D, et al. arXiv.

- [2023.07] Toolllm: Facilitating large language models to master 16000+ real-world apis [[paper]](https://arxiv.org/abs/2307.16789)

  Qin Y, Liang S, Ye Y, et al. arXiv.

- [2023.03] Mm-react: Prompting chatgpt for multimodal reasoning and action [[paper]](https://arxiv.org/abs/2303.11381)

  Yang Z, Li L, Wang J, et al. arXiv.

#### 3.4 CoT for Science üß™

- [2023.04] ChemCrow: Augmenting large-language models with chemistry tools[[paper]](https://arxiv.org/abs/2304.05376)

  Bran A M, Cox S, White A D, et al. arXiv.

- [2022.12] Large language models encode clinical knowledge [[paper]](https://www.nature.com/articles/s41586-023-06291-2)

  Singhal K, Azizi S, Tu T, et al.  Nature, 2023.

### 4. CoT Explanation

- [2021.11] An explanation of in-context learning as implicit bayesian inference [[paper]](https://arxiv.org/abs/2111.02080)

  Xie S M, Raghunathan A, Liang P, et al. arXiv.

- [2023.04] Why think step-by-step? Reasoning emerges from the locality of experience [[paper]](https://arxiv.org/abs/2304.03843)

  Prystawski B, Goodman N D. arXiv.

### 5. CoT Safety

- [2023.01] Faithful chain-of-thought reasoning [[paper]](https://arxiv.org/abs/2301.13379)

  Lyu Q, Havaldar S, Stein A, et al. IJCNLP-AACL 2023

- [2023.01] On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning [[paper]](https://aclanthology.org/2023.acl-long.244/)

  Shaikh O, Zhang H, Held W, et al. ACL 2023



## 

## Citation

